---
title: Atelier du réseau sondage
subtitle: Estimation de la variance par approche analytique - application avec **gustave**
format: 
    clean-revealjs:
        transition: slide
        smaller: true
        preview-links: true
        width: 1500
        margin: 0.05
        toc: false
        vertical: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Khaled Larbi
    email: prenom point nom arobase insee point fr
    affiliations: Insee
date: last-modified
filters:
  - webr
webr:
  packages: ['gustave']
date-format: "DD MMM YYYY"
lang: fr
bibliography: references.bib
from: markdown+emoji
---

# Pourquoi estimer la variance d'un estimateur ? 

# Estimation de variance simple
## Définitions usuelles et notations. 

Dans cette présentation, on notera :

- $\mathcal{U}$, une population finie de taille $N$,
- $p$, un plan de sondage défini sur $\mathcal{U}$,
- $S$ un échantillon aléatoire tiré selon le plan $p$.


- $\{y_k\}$ : une variable d'intérêt,
- $\{x_k\}$ : une variable auxiliaire. 

Pour l'inférence, il est commode d'introduire : 

- pour tout individu $k \in \mathcal{U}, ~ \pi_k = \mathbb{P}(k \in S)$ $\to$ probabilité d'inclusion d'ordre 1.
  - il s'agit de la probabilité que l'individu $k$ soit dans l'échantillon $S$.
- pour tout couple d'individus $(k,l) \in \mathcal{U}^2, ~ \pi_{kl} = \mathbb{P}(k \in S \cap l \in S)$ $\to$ probabilité d'inclusion d'ordre 2.
  - il s'agit de la probabilité que les individus $k$ et $l$ soient conjointement dans l'échantillon $S$



## Estimateur d'Horvitz-Thompson

:::: {.columns}

- Comment estimer le total $t_y = \sum_{k \in \mathcal{U}} y_k$ en utilisant l'information disponible uniquement sur l'échantillon $S$ ?

::: {#def-estHT}
## Estimateur d'Horvitz-Thompson
L'estimateur $\hat{t}_{y,\text{HT}} = \sum_{k \in S} \frac{y_k}{\pi_k}$ est appelé estimateur d'Hovitz-Thompson du total. 
:::

- Est-ce un *bon* estimateur ? 
- Critère de biais et de variance.



::: {.column width="50%"}
::: {.callout-caution icon="false"}
::: {#def-biais}
## Biais d'un estimation
Le biais d'un estimateur $\hat \theta$ d'une fonction d'intérêt $\theta$ est donné par $\mathbb{B}(\hat \theta; \theta) = \mathbb{E}(\hat \theta) - \theta$. 
:::
:::
:::

::: {.column width="50%"}
::: {.callout-caution icon="false"}
::: {#def-var}
## Variance
La variance d'un estimateur $\hat \theta$ d'une fonction d'intérêt $\theta$ est donné par $\mathbb{V}(\hat \theta) = \mathbb{E}(\left(\hat \theta  - \mathbb{E}(\hat \theta) \right)^2)$. 

La variance permet de quantifier la variabilité d'un estimateur.
:::
:::
:::

::::


## Arbitrage biais/variance
![](illustrations/biais_variance.svg){fig-align="center"}

## Estimateur d'Horvitz-Thompson : biais et variance

- Si pour tout $k \in \mathcal{U},~~ \pi_k > 0$ alors $\hat{t}_{y,\text{HT}}$ est un estimateur sans biais de $t_y$.

- Si il existe un individu $j$ tel que $\pi_j = 0$, alors il y a un défaut de couverture $\to$ biais. 

- Quid de la variance de $\hat{t}_{y,\text{HT}}$ ?
  - $$\mathbb{V}(\hat t_{y,\text{HT}}) = \sum_{k \in \color{red}{\mathcal{U}}} \sum_{l \in \color{red}{\mathcal{U}}} \frac{y_k}{\pi_k} \frac{y_l}{\pi_l} \Delta_{kl}$$ où $\Delta_{kl} = \pi_{kl} - \pi_k \pi_l$
  - <u>Problème</u> : la variance est fonction des valeurs de $\{y_k\}$ sur $\color{red}{\mathcal{U}}$ mais cette information n'est disponible que sur $\mathcal{S}$
  - <u>Solution</u> : estimer la variance.


:::: {.columns}

::: {.column width="50%"}
::: {.callout-caution icon="false"}
## Estimateur de variance d'Horvitz-Thompson
 $$\hat{\mathbb{V}}_\text{HT}(\hat t_{y,\text{HT}}) = \sum_{k \in \mathcal{U}} \sum_{k \in \mathcal{U}} \frac{y_k}{\pi_k} \frac{y_l}{\pi_l} \Delta_{kl} \color{red}{\frac{I_{kl}}{\pi_{kl}}} \color{black} = \sum_{k \in S} \sum_{l \in S} \frac{y_k}{\pi_k} \frac{y_l}{\pi_l} {\frac{\Delta_{kl}}{\pi_{kl}}} $$ 

 <u>Propriétés</u> : 

 - Si pour tout $(k,l) \in \mathcal{U}^2$, $\pi_{kl} > 0$, $\hat{\mathbb{V}}_\text{HT}(\hat t_{y,\text{HT}})$ est sans biais pour $\mathbb{V}_\text{HT}(\hat t_{y,\text{HT}})$
:::
:::

::: {.column width="50%"}
::: {.callout-caution icon="false"}
## Estimateur de Sen-Yates-Grundy
$$\hat{\mathbb{V}}_\text{SYG}(\hat t_{y,\text{HT}}) =  - \frac{1}{2} \sum_{k \in \mathcal{S}} \sum_{l \in \mathcal{S} | k \neq l} \left(\frac{y_k}{\pi_k} - \frac{y_l}{\pi_l} \right)^2 \frac{\Delta_{kl}}{\pi_{kl}}$$

 <u>Propriétés</u> : 
 
 - Si pour tout $(k,l) \in \mathcal{U}^2$, $\pi_{kl} > 0$ et le plan $p$ est de taille fixe alors $\hat{\mathbb{V}}_\text{SYG}(\hat t_{y,\text{HT}})$ est sans biais pour $\mathbb{V}_\text{HT}(\hat t_{y,\text{HT}})$.
- Si pour tout $(k,l) \in \mathcal{U}^2$, $\pi_{kl} - \pi_k \pi_l \leq 0$ alors $\hat{\mathbb{V}}_\text{SYG}(\hat t_{y,\text{HT}}) \geq 0$.
:::
:::
::::


## Application à quelques plans de sondage.

::: {.callout-caution icon="false"}
## Plan aléatoire simple sans remise 
$p_{\text{SASSR}(n;N)}$ est un plan aléatoire simple sans remise de taille $n$ parmi $N$ si :

- tous les échantillons de taille $n$ ont même probabilité d'être tiré,
- si tous les échantillons de taille différente de $n$ ont une probabilité nulle d'être tiré.

Si $S \sim $p_{\text{SASSR}(n;N)}$ alors pour tout $(k,l) \in \mathcal{U}^2, ~~ \pi_k = \frac{n}{N}$ et si $k \neq l~~$, $\pi_{kl} = \frac{n(n-1)}{N(N-1)}$

<u>Application</u> : $\hat{\mathbb{V}}_\text{SYG}(\hat t_{y,\text{HT}}) = \hat{\mathbb{V}}_\text{HT}(\hat t_{y,\text{HT}}) = \frac{N^2}{n} (1 - \frac{n}{N}) s^2_y$ où $s^2_y$ est la dispersion de la variable $\{y_k\}$ sur l'échantillon $S$.
:::




```{webr-r}
n <- 100
N <- 10000
y <- 1:n
pik <- rep(n/N, n)
gustave::var_srs(y = y, pik =  pik, strata = NULL, w = NULL, precalc = NULL)
#Le plan est de taille fixe, il est donc possible d'utiliser l'estimateur de Sen-Yates-Grundy

#Définition de la matrice des probabilités doubles
pikl <- matrix(n*(n-1)/(N*(N-1)), ncol = n, nrow = n)
diag(pikl) <- pik
gustave::varSYG(y = y, pikl = pikl)
```

##

::: {.callout-caution icon="false"}
## Plan poissonien
$p_{\text{Poisson}(\textbf{p})}$ où $\textbf{p} = (p_1, ..., p_N) \in [0;1]^N$ est un plan de Poisson si :

- tous les individus sont tirés indépendamment les uns des autres, 
- la probabilité d'un individu $k$ soit dans l'échantillon est $p_k$.

Si $S \sim $p_{\text{Poisson}(\textbf{p})}$ alors pour tout $(k,l) \in \mathcal{U}^2, ~~ \pi_k = \textbf{p}_k$ et si $k \neq l~~$, $\pi_{kl} = \textbf{p}_k \textbf{p}_l$

<u>Application</u> : $\displaystyle \hat{\mathbb{V}}_\text{HT}(\hat t_{y,\text{HT}}) = \sum_{k \in S} y_k^2 \frac{1-\textbf{p}_k}{\textbf{p}_k}$
:::

```{webr-r}
n <- 100
y <- 1:n
pik <- rep(0.5, n)
gustave::var_srs(y = y, pik =  pik, w = NULL)
```

## Approximations classiques

- Parfois, il est délicat de calculer les probabilités d'inclusion d'ordre 2.

- Pour certains plans, des approximations sont utilisées.

- Pour les plans équilibrés à forte entropie : formule de Deville-Tiilé (@varDT).
$$\hat{\mathbb{V}}_\text{DT}(\hat{t}_{y,\text{HT}}) = \sum_{k \in S} (1 - \pi_k) \left(\frac{y_k}{\pi_k} - \frac{x_k}{\pi_k} \hat{\beta} \right)^2$$ où $\hat{\beta}$ est le coefficient estimé de la régression de $\{\frac{y_k}{\pi_k}\}$ par $\{\frac{x_k}{\pi_k}\}$ pondérée par $1-\pi_k$. 
  - Remarque : le plan aléatoire simple sans remise $p_{\text{SASSR}(n;N)}$ est un plan équilibré sur la variable $\{pi_k\}$ $\to$ il est donc possible d'utiliser l'approximation de Deville-Tillé et $\hat{\mathbb{V}}_\text{DT}(\hat{t}_{y,\text{HT}}) = \hat{\mathbb{V}}_\text{SYG}(\hat{t}_{y,\text{HT}}) = \hat{\mathbb{V}}_\text{HT}(\hat{t}_{y,\text{HT}})$



- Pour le tirage systématique : approximation par la variance sous un plan aléatoire simple. 


## 
Plan de l'enquête Histoire de Vie et Patrimoine

::: fragment
-   <ins>Idée 1</ins> : utiliser les résultats présentés. :x:
:::


::: fragment
-   <ins>Problème 1</ins>  : le calcul des probabilités d'inclusion d'ordre deux est (très)
        délicat.
:::

::: fragment
-   <ins>Idée 2</ins> : estimer les probabilités d'inclusion d'ordre deux par
        simulation. :x: 
:::

::: fragment
-   <ins>Problème 2</ins> : l'estimation des probabilités d'inclusion d'ordre deux est très
        gourmande en ressource informatique et peut conduire à des
        estimations instables.
:::

::: fragment
-   <ins>Idée 3</ins> : décomposer le calcul de l'estimateur de la variance degré par degré. :white_check_mark:
:::

::: fragment
-   <ins>Idée 4</ins> : utiliser des méthodes de réplication. :white_check_mark:
:::

## Estimation de variance pour des plans à plusieurs degrés (@Rao75, Caron et al 1998)

:::: {.columns}

::: {.column width="50%"}

Soit $p$ un plan de sondage à deux degrés où $p_1$ désigne le plan de
sondage des unités primaires et $p_{2|1}$ le plan de sondage des unités
secondaires conditionnellement aux unités primaires. On notera
respectivement $s_\text{UP}$ et $s_\text{US}$, les échantillons d'unités
primaires et secondaires obtenus.

On note : 

- $\pi_u^{(1)}$, les probabilités d'inclusion d'ordre 1 d'une
unité primaire $u \in U_\text{UP}$. 
- $\pi_l^{(2|1)}$, les probabilités
d'inclusion d'ordre 1 d'une unité secondaire $l \in U_\text{UP}$
conditionnellement à l'échantillon des UP.
- $\pi_l^{(1,2)}$, les
probabilités d'inclusion d'ordre 1 d'une unité secondaire
$l \in U_\text{UP}$.

Soit $\hat y$, l'estimateur d'Horvitz-Thompson d'une variable d'intérêt
$y$ :
$$\hat y = \sum_{k \in S_{\text{US}}} \frac{y_k}{\pi_k^{(1,2)}} = \sum_{u \in S_\text{UP}} \frac{\hat{y}_{(u)}}{\pi_u^{(1)}} \text{  avec  } \hat{y}_{(u)} = \sum_{k \in u \cap S_\text{US}} \frac{y_k}{\pi_k^{(2|1)}}$$
:::

::: {.column width="50%"}

Supposons que :

-   le tirage des unités secondaires au sein des unités primaires sont
    indépendants d'une unité primaire à l'autre.
-   l'existence d'un estimateur $\hat{\mathbb{V}^{1}}(\hat y)$ sans
    biais de la variance lié au premier degré d'échantillonnage
    $\mathbb{V}^{1}(\hat y)$ pouvant s'écrire sous la forme
    $\displaystyle \hat{\mathbb{V}} (\hat y) = Q(y_{(1)}, ..., y_{(n_\text{UP})}) = \sum_{i} \sum_{j \neq i} q_{ij} y_{(i)} y_{(j)} + \sum_{i} q_{i} y_{(i)}^2$
    .
-   pour tout $u \in \mathcal{U}_\text{UP}$, l'existence d'un estimateur
    sans biais $\hat y_{(u)}$ (sous le deuxième degré) de
    $\displaystyle y_{(u)} = \sum_{k \in u} y_k$
-   pour tout $u \in \mathcal{U}_\text{UP}$, l'existence d'un estimateur
    sans biais $\hat{\mathbb{V}}^{2|1}(\hat{y}_{(u)})$ (sous le deuxième
    degré) de la variance de l'estimateur $\hat y_(u)$.

Sous ces hypothèses, la variance totale de l'estimateur
d'Horvitz-Thompson sous le plan $p$ peut être estimée sans biais par
$$\hat{\mathbb{V}}^{(1,2)}(\hat y_{(u)}) = \hat{\mathbb{V}}^{(1,2)}(\{ y_{l} : l \in S_\text{US} \}) =  Q(\hat{y}_{(1)}, ..., \hat{y}_{(n_\text{UP})}) + \sum_{u \in S_\text{UP}} \left( \frac{1}{(\pi_u^{(1)})^2} - q_u \right) \hat{\mathbb{V}}^{2|1}(\hat{y}_{(u)}) $$
:::
:::
::::

## Récaptulatif

# Estimation de variance par linéarisation 

## Estimation par subtitution

- Nous savons calculer la variance d'estimateurs d'Horvitz-Thompson $\hat{t}_{y,\text{HT}} = \sum_{k \in S} \frac{y_k}{pi_k}$ du total.

- En pratique, on s'intéresse à d'autres paramètres (univariés) : $\theta = f(t_{y^{(1)}}, ..., t_{y^{(d)}})$
  - Exemple 1 : la moyenne de la variable $\mu_{y^{(1)}} = \frac{1}{N} t_{y^{(1)}}$
  - Exemple 2 : le rapport des totaux des variables $\{y_1\}$ $\{y_2\}$ $R_{\mu_{y^{(1)}}, \mu_{y^{(2)}}} = \frac{t_{y^{(1)}}}{t_{y^{(2)}}}$
  
- Comment trouver un estimateur (approximativement) sans biais ?
  
- Principe de subtitution : on remplace les paramètres inconnus par des estimations.
  - Exemple 1 : la moyenne de la variable $\frac{1}{N} t_{y^{(1)}}$
  - Exemple 2 : la moyenne de la variable $\frac{1}{N} t_{y^{(1)}}$

- Quid de la variance ?

## Principe de linéarisation

- Le principe de linéarisation va permettre d'estimer la variance d'estimateurs par subtitution.

- L'idée est d'approximer la variance de l'estimateur par subtitution $\hat{\theta} = \mathbb{V}f(\hat{t}_{y^{(1),\text{HT}}, \text{HT}}, ..., \hat{t}_{y^{(d),\text{HT}}})$ par celle de l'estimateur du total d'Horvitz-Thompson d'une variable $\mathbb{V}(\hat{t}_{u,\text{HT}})$.

- Comment construire $\{u_k\}$ ? 

- Plusieurs approches différentes :
  - Si $\hat{\theta} = \mathbb{V}f(\hat{t}_{y^{(1),\text{HT}}, \text{HT}}, ..., \hat{t}_{y^{(d),\text{HT}}})$ où $f$ est différentiable. 
  - Si $\hat{\theta}$ est solution d'une équation estimante.
  - Si $\hat{\theta}$ est une fonctionnelle. 


## Approche 1 : si $f$ différentiable

- Supposons de plus que $f$ est régulière : $f$ différentiable. 

::: {.fragment}
- En utilisant un développement de Taylor (cas où $f : \mathbb{R} \to \mathbb{R}$): 
$$f(\hat{t}_{y,\text{HT}}) \approx f(t_y) + (\hat{t}_{y,\text{HT}} - t_y) f'(t_y)$$
:::


::: {.fragment}
- Par passage à l'espérance :
$$\mathbb{E}(f(\hat{t}_{y,\text{HT}})) \approx \mathbb{E}(f(t_y)) + \mathbb{E}((\hat{t}_{y,\text{HT}} - t_y) f'(t_y)) \approx \mathbb{E}(f(t_y))$$
$\to$ <span style="color: red;">Si l'estimateur d'Horvitz-Thompson $\hat{t}_{y,\text{HT}}$ est sans biais pour $t_y$ alors $f(t_{y,\text{HT}})$ l'est approximativement pour $f(t_{y})$. </span>
:::

::: {.fragment}
- Par passage à la variance :
$$\mathbb{V}(f(\hat{t}_{y,\text{HT}})) \approx \mathbb{V}((\hat{t}_{y,\text{HT}} - t_y) f'(t_y)) \approx  \color{red}{\mathbb{V}((\hat{t}_{ f'(t_y) \times y,\text{HT}}))}$$
:::

::: {.fragment}
::: {#thm-biasub}
## Biais de l'estimateur par subtitution

Si l'estimateur d'Horvitz-Thompson $\hat{t}_{y,\text{HT}}$ est sans biais pour $t_y$ alors $\mathbb{E}(f(\hat{t}_{y,\text{HT}})) - f(t_y) \approx 0$.
:::
:::

## Approche 1 : si $f$ différentiable

::: {.fragment}
- La variance d'un estimateur de la forme $f(\hat t_{y,\text{HT}})$ est approximativement égale à la variance de l'estimateur du total de la variable $u$ définie pour tout individu $k, ~~ u_k = f'(t_y) \times y_k$ : $$ \mathbb{V}(f(\hat t_{y,\text{HT}})) \approx \mathbb{V}(\hat t_{u,HT}) $$
:::

::: {.fragment}
- Intuitivement : la variance de $f(\hat t_{y,\text{HT}})$ est approximativement la même que celle d'un estimateur d'Horvitz-Thompson pour une variable d'intérêt bien choisie $\to$ rôle central de l'estimation du total. 
:::

::: {.fragment}
- La variable $\{u_k\}_{k \in \mathcal{U}}$ est appelée *variable linéarisée* associée à $f$.
:::

::: {.fragment}
- Problème : cette variable est définie par pour tout individu $k \in \mathcal{U} ~~ u_k = f'(\color{red}{t_y}\color{black}) \times y_k \to$ il est donc nécessaire de connaître $\displaystyle t_y = \sum_{k \in \mathcal{U}} y_k$ qui est inconnu.
:::

::: {.fragment}
- Solution : estimer par subtitution la variable linéarisée $\{u_k\}$.
    - Pour l'individu $k \in \mathcal{U}$, $u_k = f'(t_y) \times y_k$ sera estimé par $\hat{u}_k = f'(\hat{t}_{y,\text{HT}}) \times y_k$.
:::

::: {.fragment}
- La variable $\hat{u}_k$ est la variable linéarisée estimée.
:::


::: {.fragment}
::: {#thm-lin}
## Estimation de la variance par linéarisation - cas unidimensionnel

L'estimateur de la variance par linéarisation d'une fonction d'intérêt de la forme $f(t_{y})$ est donné par $\displaystyle \hat{\mathbb{V}}_\text{lin}(f(\hat{t}_{y,\text{HT}})) = \mathbb{V}(\hat t_{\hat{u},HT})$
:::
:::


## Exemple de linéarisation - cas unidimensionnel


::: {.fragment}
- On suppose que l'échantillon $s$ dont nous disposons est tiré selon un plan de sondage tel que pour tout individu $k \in \mathcal{U}, ~ \pi_k > 0$
$\to$ l'estimateur du total d'Horvitz-Thompson $\hat{t}_{y,\text{HT}}$ est un estimateur sans biais de $t_y$.  
:::

::: {.fragment}
- Dans cet exemple, la variable d'intérêt prend des valeurs strictement positives. 
:::


::: {.fragment}
- Nous souhaitons estimer $\log{t_y}$ : un estimateur par subtitution est donné par $\log{\hat{t}_{y,\text{HT}}}$.
:::


::: {.fragment}
- Cet estimateur est approximativement sans biais pour $\log{t_y}$ car $\hat{t}_{y,\text{HT}}$ est sans biais pour $t_y$.
:::

::: {.fragment}
- L'estimateur de la variance par linéarisation est donné par $\mathbb{V}(\hat t_{\hat{u},HT})$ où pour tout $k \in \mathcal{U}$, $\hat{u}_k = \frac{y_k}{\hat{t}_{y,\text{HT}}}$.

Il reste à utiliser les résultats propres au plan de sondage afin de déterminer un estimateur de la variance. 
:::


## Simulation

A FAIRE


## Linéarisation d'une fonction de plusieurs totaux

::: {.fragment}
- Il est possible d'utiliser cette approche pour des fonctions de totaux de plusieurs variables d'intérêt $f(t_{y^1}, ..., t_{y^d})$ où $f : \mathbb{R}^d \to \mathbb{R}$ est une fonction différentiable.
:::

::: {.fragment}
- Principe de subtitution : un estimateur de $f(t_{y^1}, ..., t_{y^d})$ est donné par $f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d,\text{HT}}})$.
:::

::: {.fragment}
- Même idée : en utilisant la formule de Taylor $$  f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d,\text{HT}}}) \approx f(t_{y^1}, ..., t_{y^d}) +  \nabla f(t_{y^1}, ..., t_{y^d})( \hat{t}_{y^1, \text{HT}} - t_{y^1} , ..., \hat{t}_{y^d,\text{HT}} - t_{y^d})^T  $$
:::

::: {.fragment}
- Il est possible d'obtenir l'approximation de la variance suivante :
$$\mathbb{V}(f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d,\text{HT}}})) \approx \mathbb{V}(\nabla f(t_{y^1}, ..., t_{y^d})  ( \hat{t}_{y^1, \text{HT}}  , ..., \hat{t}_{y^d,\text{HT}})^T) = \hat{t}_{u,\text{HT}}$$ 

où la variable $\{u_k\}_{k \in \mathcal{U}}$ est définie pour tout $k \in \mathcal{U}$ par $u_k = \nabla f(t_{y^1}, ..., t_{y^d}) (y^1_k, ..., y^d_k)^T$
:::

::: {.fragment}
- $u_k$ est la variable linéarisée associée à $f$ $\to$ même problème que dans les cas univarié : pas accès aux totaux.
:::

## Linéarisation d'une fonction de plusieurs totaux (2)

::: {.fragment}
- $u_k$ est la variable linéarisée associée à $f$ $\to$ même problème que dans les cas univarié : pas accès aux totaux.

$\to$ utilisation de la variable linéarisée estimée : $\hat u_k = \nabla f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{y^d,\text{HT}}) (y^1_k, ..., y^d_k)^T$.
:::

::: {.fragment}
- Estimation par subtitution : $\hat{V}_\text{lin}(f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d,\text{HT}}})) =  \mathbb{V}( \nabla f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d},\text{HT}}) ( \hat{t}_{y^1, \text{HT}} , ..., \hat{t}_{y^d,\text{HT}} )^T) = \mathbb{V}(\hat{t}_{\hat{u},\text{HT}})$
:::

::: {.fragment}
::: {#thm-linmul}
## Estimation de la variance par linéarisation

L'estimateur de la variance par linéarisation d'une fonction d'intérêt de la forme $f(t_{y^1}, ..., t_{y^d})$ est donné par $\displaystyle \hat{\mathbb{V}}_\text{lin}(f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d,\text{HT}}})) = \mathbb{V}( \nabla f(\hat{t}_{y^1,\text{HT}}, ..., \hat{t}_{{y^d},\text{HT}}) ( \hat{t}_{y^1, \text{HT}} , ..., \hat{t}_{y^d,\text{HT}})^T)$
:::
:::

## Exemple de linéarisation

- Supposons que nous disposons de deux variables d'intérêt $y^{(1)}$ (par exemple, la part de transferts sociaux) et $y^{(2)}$ (par exemple, le revenu total) et que nous souhaitons connaître un estimateur du ratio des totaux $R_{y^{(1)}, y^{(2)}} = \frac{t_{y^{(1)}}}{t_{y^{(2)}}}$.

- L'estimateur par subtitution de $R_{y^{(1)}, y^{(2)}}$ noté $\hat{R}_{\y^{(1)}, y^{(2)}, \text{sub}}$ est donné par $\hat{R}_{y^{(1)}, y^{(2)}, \text{sub}} = \frac{\hat{t}_{y^{(1)},\text{HT}}}{\hat{t}_{y^{(2)},\text{HT}}}$ .

- Cet estimateur est approximativement sans biais (le biais est d'autant plus faible que les estimateurs des totaux ont une faible variance et que la fonction d'intérêt *ne fluctue pas trop* - on suppose que pour tout $k \in \mathcal{U}, \pi_k > 0$).

- Quid de la variance ? 
    - Malheureusement, $\mathbb{V}(\hat{R}_{\text{sub}}) = \mathbb{V}(\frac{\hat{t}_{y^{(1)},\text{HT}}}{\hat{t}_{y^{(2)},\text{HT}}}) \neq  \frac{\mathbb{V}(\hat{t}_{y^{(1)},\text{HT}})}{\mathbb{V}({\hat{t}_{y^{(2)},\text{HT}})}}$
    - Utilisation du principe de substitution.

## Exemple de linéarisation (2)

- $f : (x,y) \in \mathbb{R} \times \mathbb{R}^* \to \frac{x}{y}$
- Pour tout $(x,y) \in \mathbb{R} \times \mathbb{R}^*$, $\nabla f(x,y) = (\frac{1}{y}, \frac{-x}{y})$

- La variable linéarisée pour un individu $k$ vaut donc $u_k = \frac{y^{(1)}_k}{t_{y^{(2)}}} - \frac{t_{y^{(1)}}}{t_{y^{(2)}}^2} y^{(2)}_k = \frac{1}{t_{y^{(2)}}} (y^{(1)}_k - R_{y^{(1)}, y^{(2)}} y^{(2)}_k)$

- La variable linéarisée estimée pour un individu $k$ vaut $\hat{u}_k =  \frac{1}{\hat{t}_{y^{(2)},\text{HT}}} (y^{(1)}_k - \hat{R}_{y^{(1)}, y^{(2)},\text{sub}} y^{(2)}_k)$


L'estimateur de la variance par $\mathbb{V}(\hat{R}_{y^1, y^2, \text{sub}})$ est approximativement $\mathbb{V(\hat{t}_{\hat{u}, \text{HT}})}$ où $\hat{u}_k =  \frac{1}{\hat{t}_{y^{(2)},\text{HT}}} (y^{(1)}_k - \hat{R}_{y^{(1)}, y^{(2)},\text{sub}} y^{(2)}_k)$.


## Linéarisation par fonction objective

- Certains paramètres d'intérêt et estimateurs sont solution d'une équation estimante.

- Dans certains cas, une expression close du paramètre ou de l'estimateur n'existe pas.
  - Formellement, il n'est pas possible d'écrire simple 

- Il est cependant possible d'obtenir une estimation de la variance de l'estimateur par linéarisation de la fonction objective.

- Soit $\theta \in \mathbb{R}^d$, un paramètre tel que 
\begin{equation} 
\frac{1}{N} \sum_{k \in \mathcal{U}} \phi(\theta; x_k, y_k) = 0 \text{  où  } \phi : \mathbb{R}^d \to \mathbb{R}^{d} \text{ différentiable.}
\label{defparameq}
\end{equation}

  - Exemple 1 : la moyenne $\displaystyle {\mu}_y = \frac{1}{N} \sum_{k \in \mathcal{U}} y_k$ avec $\phi(\theta; x_k, y_k) = y_k - \theta$
  - Exemple 2 : le coefficient de régression $\beta$ de la régression de $y$ sur $x$ avec $\phi(\theta; x_k, y_k) = x_k (y_k - x_k^T \theta)$

- Il est possible de construire un estimateur associé $\hat{\theta}$ tel que
\begin{equation} 
\frac{1}{N} \sum_{k \in S} \frac{\phi(\hat{\theta}; x_k, y_k)}{\pi_k} = 0 \text{  où  } \phi : \mathbb{R}^d \to \mathbb{R}^{d} \text{ différentiable.}
\label{defestimeq}
\end{equation}

  - Exemple 1 : l'estimateur $\displaystyle \hat{\mu}_{y, \text{Hayek}} = \frac{1}{\sum_{k \in S}\frac{1}{\pi_k}} \sum_{k \in S} \frac{y_k}{\pi_k}$ avec $\phi(\theta; x_k, y_k) = y_k - \theta$
  - Exemple 2 : le coefficient de régression estimée $\displaystyle \hat{\beta} = \left(\sum_{k \in S} \frac{x_k x_k^T}{\pi_k} \right)^{-1} \sum_{k \in S} \frac{x_k y_k}{\pi_k}$ de la régression de $y$ sur $x$ avec $\phi(\theta; x_k, y_k) = x_k (y_k - x_k^T \theta)$

## Intuition de la linéarisation par fonction estimante

- Si $\phi$ est différentiable,  $$\phi(\hat{\theta}; x_k, y_k) \approx \phi(\theta; x_k, y_k) + (\text{Jac}(\phi)(\theta)) (\hat{\theta} - \theta)$$

- D'où $$\sum_{k \in S} \frac{\phi(\hat{\theta}; x_k, y_k)}{\pi_k} \approx \sum_{k \in S} \frac{\phi(\theta; x_k, y_k)}{\pi_k} + (\sum_{k \in S} \frac{\text{Jac}(\phi)(\theta; x_k, y_k))}{\pi_k}) (\hat{\theta} - \theta)$$

- Or $\sum_{k \in S} \frac{\phi(\hat{\theta}; x_k, y_k)}{\pi_k} = 0$

- D'où
\begin{align}
(\hat{\theta} - \theta) &\approx \sum_{k \in S}  (-\sum_{j \in S} \frac{\text{Jac}(\phi)(\theta; x_j, y_j))}{\pi_j})^{-1} \frac{\phi(\theta; x_k, y_k)}{\pi_k} \\
&\approx \sum_{k \in S}  (-\sum_{j \in \mathcal{U}} {\text{Jac}(\phi)(\theta; x_j, y_j))})^{-1} \frac{\phi(\theta; x_k, y_k)}{\pi_k} \\
&\approx \sum_{k \in S} \frac{u^\text{est}_k(\theta)}{\pi_k}
\end{align}



::: {#thm-blineq}
## Estimateur de la variance par linéarisation de l'équation estimante

L'approximation suivante $\displaystyle \mathbb{V}(\hat{\theta}) \approx \mathbb{V}(\sum_{k \in S} \frac{u^\text{est}_k(\theta)}{\pi_k})$ permet de créer un estimateur $\displaystyle \hat{\mathbb{V}}(\hat{\theta}) \approx \hat{\mathbb{V}}(\sum_{k \in S} \frac{\widehat{u^\text{est}_k(\theta)}}{\pi_k})$
:::
## Exemple de linéarisation par fonction estimante

- La variable $\{ u^\text{est}_k(\theta) \}$ est la variable linéarisée associée à l'estimateur $\hat{\theta}$ $\to$ $\theta$ est inconnu.

- La variable $\{ u^\text{est}_k(\hat{\theta}) \}$ est la variable linéarisée estimée associée à l'estimateur.

- Pour des développements plus rigoureux :
  - Preuve de la normalité asymptotique des Z-estimateurs (iid) : 
  - Preuve de la normalité asymptotique des Z-estimateurs (cas sondage) : 


## Exemple d'application

- Exemple 2 : le coefficient de régression estimée $\displaystyle \hat{\beta} = \left(\sum_{k \in S} \frac{x_k x_k^T}{\pi_k} \right)^{-1} \sum_{k \in S} \frac{x_k y_k}{\pi_k}$ de la régression de $y$ sur $x$ avec $\phi(\theta; x_k, y_k) = x_k (y_k - x_k^T \theta)$

- $\text{Jac}(\phi)(\theta; x_j, y_j) = x_k x_k^T$ 

- $u_k^\text{est}(\theta) = \left(- \sum_{k \in \mathcal{U}} x_j x_j^T \right)^{-1} {x_k}({y_k - x_k^T \theta})$

- Application de l'estimateur $\displaystyle \hat{\mathbb{V}}$ de la variance d'un total à $\widehat{u_k^\text{est}(\hat{\beta})} = \left(- \sum_{k \in S} \frac{x_j x_j^T}{\pi_k} \right)^{-1} {x_k}({y_k - x_k^T \hat{\beta}})$


::: {.callout-important}
- Il s'agit d'une version allégée de la linéarisation sur équation estimante : beaucoup d'hypothèses ont été cachées.
- Ces hypothèses sont parfois techniques et difficilement vérifiables en pratique.
- Ne pas hésitez à vérifier l'implémentation des estimations par linéarisation sur des exemples simples en utilisant des simulations. 
:::



# Prise en compte des redressements dans l'estimation de variance



# Logique de `gustave`